{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyphen12/quabity-ashuance/blob/main/Data_Quality_sanitized%5Bmodular%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9df15392",
      "metadata": {
        "id": "9df15392"
      },
      "source": [
        "# Data Quality Report\n",
        "\n",
        "All functions.\n",
        "\n",
        "- Run first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f88cad29",
      "metadata": {
        "id": "f88cad29"
      },
      "outputs": [],
      "source": [
        "import os, yaml\n",
        "\n",
        "os.environ['http_proxy'] = \"http://proxy-dmz.intel.com:911\" \n",
        "os.environ['https_proxy'] = \"http://proxy-dmz.intel.com:912\" \n",
        "os.environ['no_proxy'] = \"intel.com,.intel.com,10.0.0.0/8,192.168.0.0/16,localhost,::1,.local,127.0.0.0/8,134.134.0.0/16,172.16.0.0/12\"\n",
        "\n",
        "#!pip install pyodbc\n",
        "#!pip install missingno\n",
        "#!pip install dataframe_image\n",
        "#!pip install fpdf\n",
        "#!pip install pdfkit\n",
        "#!pip install pandas-profiling\n",
        "\n",
        "import pyodbc\n",
        "from time import time\n",
        "from datetime import timedelta\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import datetime\n",
        "import string\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "from collections import Counter\n",
        " \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import unravel_index\n",
        "import missingno as msno\n",
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "import dataframe_image as dfi\n",
        "from fpdf import FPDF\n",
        "import pdfkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a90045de",
      "metadata": {
        "id": "a90045de"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    sourcedata1 = pd.read_csv('sourcedata1.csv')\n",
        "    sourcedata2 = pd.read_csv('sourcedata2.csv')\n",
        "    return sourcedata1, sourcedata2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4eac8e5",
      "metadata": {
        "id": "f4eac8e5"
      },
      "outputs": [],
      "source": [
        "def load_data_multiple():\n",
        "    import os\n",
        "    arr = os.listdir('files')\n",
        "    \n",
        "    datalst = []\n",
        "    \n",
        "    id = 'Process ID'\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    for filename in arr:\n",
        "        flname = 'files/' + filename\n",
        "        df = pd.read_csv(flname)\n",
        "        df[id] = df[id].astype(float)\n",
        "        datalst.append(df)\n",
        "        print(filename)\n",
        "    return datalst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b77a9f9c",
      "metadata": {
        "id": "b77a9f9c"
      },
      "outputs": [],
      "source": [
        "def data_format_multiple(df_list):\n",
        "    #df1 = sourcedata1\n",
        "    #df2 = sourcedata2\n",
        "    id = 'Process ID'\n",
        "    #df1[id] = df1[id].astype(float)\n",
        "    #df2[id] = df2[id].astype(float)\n",
        "    #Merge sourcedata\n",
        "    ##################Request: for all dfs available, not just 1 or 2\n",
        "    # del data\n",
        "    #data = df1.merge(df2, how='left', on='Process ID')\n",
        "    data = pd.concat(df_list)\n",
        "    #Show available data columns\n",
        "    data.columns\n",
        "    #Select columns for data quality analysis (in main file)\n",
        "    cols = ['Process ID', 'Process Name', 'Process Purpose', 'Product Line', 'Organization Name',\n",
        "        'HRGO Process', 'Execution Frequency', 'Automation Level',\n",
        "        'Financial Impact','Employees Served', 'Executives Served', 'Region', 'Country', 'DFE EE Population', 'DFE VP Population', 'Modified', 'Created']\n",
        "    #Final data for profiling\n",
        "    data = data[cols]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b35fc5",
      "metadata": {
        "id": "14b35fc5"
      },
      "outputs": [],
      "source": [
        "def data_format(df1, df2):\n",
        "    #df1 = sourcedata1\n",
        "    #df2 = sourcedata2\n",
        "    id = 'Process ID'\n",
        "    df1[id] = df1[id].astype(float)\n",
        "    df2[id] = df2[id].astype(float)\n",
        "    #Merge sourcedata\n",
        "    ##################Request: for all dfs available, not just 1 or 2\n",
        "    # del data\n",
        "    data = df1.merge(df2, how='left', on='Process ID')\n",
        "    #Show available data columns\n",
        "    data.columns\n",
        "    #Select columns for data quality analysis (in main file)\n",
        "    cols = ['Process ID', 'Process Name', 'Process Purpose', 'Product Line', 'Organization Name',\n",
        "        'HRGO Process', 'Execution Frequency', 'Automation Level',\n",
        "        'Financial Impact','Employees Served', 'Executives Served', 'Region', 'Country', 'DFE EE Population', 'DFE VP Population', 'Modified', 'Created']\n",
        "    #Final data for profiling\n",
        "    data = data[cols]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb7fc8e",
      "metadata": {
        "id": "9bb7fc8e"
      },
      "outputs": [],
      "source": [
        "def clean_data(data):\n",
        "    # Replace other kinds of notations and blank fields for null values (nan):\n",
        "    null_values = ['NA', '?', 'Nil','None', '', 'NULL']\n",
        "    #Replace all the null values with na\n",
        "    data = data.replace(null_values, np.nan)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef5560dc",
      "metadata": {
        "id": "ef5560dc"
      },
      "outputs": [],
      "source": [
        "def profiling_by_dimension(data):\n",
        "    #Completeness\n",
        "    #General completeness of data\n",
        "    completeness = (data.notna().sum().sum())/(data.count().sum())*100\n",
        "\n",
        "    #Completeness per column\n",
        "    data_completeness = pd.DataFrame(\n",
        "        data.notna().sum(),\n",
        "        columns = ['Completeness']\n",
        "    )\n",
        "    #Validity\n",
        "    #Validity per column\n",
        "    data_type = pd.DataFrame(\n",
        "        data.dtypes,\n",
        "        columns = ['Data Format']\n",
        "    )\n",
        "    \n",
        "    #############Request:To check validity, we need to assign an ideal data type for each column in data. can we do this in a simpler way?\n",
        "    types = [\n",
        "    #Process ID\n",
        "    'float64',\n",
        "    #Process Name\n",
        "    'object',\n",
        "    #Process Purpose\n",
        "    'object',\n",
        "    #Product Line\n",
        "    'object',\n",
        "    #Organization Name\n",
        "    'object',\n",
        "    #HRGO Process\n",
        "    'object',\n",
        "    #Execution Frequency\n",
        "    'object',\n",
        "    #Automation Level\n",
        "    'object',\n",
        "    #Financial Impact\n",
        "    'float64',\n",
        "    #Employees Served\n",
        "    'int64',\n",
        "    #Executives Served\n",
        "    'int64',\n",
        "    #Region\n",
        "    'object',\n",
        "    #Country\n",
        "    'object',    \n",
        "    #DFE EE Population\n",
        "    'float64', \n",
        "    #DFE VP Population\n",
        "    'float64',\n",
        "    #Created\n",
        "    'datetime64[ns]',\n",
        "    #Modified\n",
        "    'datetime64[ns]',\n",
        "    ]\n",
        "\n",
        "    data_validity = pd.DataFrame(\n",
        "        {'Expected Format': types},\n",
        "        index=data_type.index\n",
        "    )\n",
        "    #Validity per column\n",
        "    data_validity = data_type.join(data_validity)\n",
        "    data_validity['Validity %'] = np.where((data_validity['Data Format'] == data_validity['Expected Format']), 100, 0)\n",
        "    #General Validity\n",
        "    validity = round(data_validity['Validity %'].mean(),2)\n",
        "    #Nullity-Missing values\n",
        "    #General nullity for data\n",
        "    nullity = round((data.isna().sum().sum())/(data.count().sum())*100,2)\n",
        "    #Nullity per colum\n",
        "    missing_data = pd.DataFrame(\n",
        "        data.isnull().sum(), #To see by percentage /data.sum()*100\n",
        "        columns=['Missing Values']\n",
        "    )\n",
        "    #Missing Data Graph\n",
        "    fig = msno.matrix(data, sparkline=False, figsize=(10,5), fontsize=12, color=(0.27, 0.52, 1.0))\n",
        "    fig_copy = fig.get_figure()\n",
        "    fig_copy.savefig('missing.png', bbox_inches = 'tight')\n",
        "    #Uniqueness\n",
        "    #General duplicate identification\n",
        "    duplicates = 100 - (data.duplicated().sum()/len(data)*100)\n",
        "\n",
        "    #Duplicates by column\n",
        "    unique_values = pd.DataFrame(\n",
        "        columns=['Unique Values']\n",
        "    )\n",
        "    for row in list(data.columns.values):\n",
        "        unique_values.loc[row] = [data[row].nunique()]\n",
        "        \n",
        "    #Zero Values\n",
        "    #Zero Values by column\n",
        "    zero_values = pd.DataFrame(\n",
        "        data.eq(0).sum(), #Para ver por porcentaje /data.sum()*100\n",
        "        columns=['Zero Values']\n",
        "    )\n",
        "    \n",
        "    #Minimum values by column\n",
        "    minimum_values = pd.DataFrame(\n",
        "        columns=['Minimum Value']\n",
        "    )\n",
        "    for row in list(data.columns.values):\n",
        "        minimum_values.loc[row] = [data[row].dropna().min()]\n",
        "        \n",
        "    #Maximum values by column\n",
        "    maximum_values = pd.DataFrame(\n",
        "        columns=['Maximum Value']\n",
        "    )\n",
        "    for row in list(data.columns.values):\n",
        "        maximum_values.loc[row] = [data[row].dropna().max()]\n",
        "        \n",
        "    #Timeliness\n",
        "    #Timeliness is intented as data updated over the past two years from present.  Only available for entire data, not by column.\n",
        "    from datetime import date\n",
        "    data['Year'] = pd.DatetimeIndex(data['Modified']).year\n",
        "    data['Updated'] = np.where((data['Year'] >= date.today().year-2), 1, 0) #We consider updated everything modified in current or past year)\n",
        "\n",
        "    timeliness = data['Updated'].sum()/len(data)*100\n",
        "    \n",
        "    #Create table with Process IDs for checking\n",
        "    #We select process with high delta between dfe population and employees served as candidates for updating.\n",
        "    data['EE Delta'] = (data['DFE EE Population']-data['Employees Served'])/data['DFE EE Population']\n",
        "    data['Check EE Population'] = np.where(((data['EE Delta'] >= 0.5) | (data['EE Delta'] <= -0.5)), 1, 0) #We consider updated everything modified in current or past year)\n",
        "    check_ee = data['Check EE Population'].sum()\n",
        "\n",
        "    data['VP Delta'] = (data['DFE VP Population']-data['Executives Served'])/data['DFE VP Population']\n",
        "    data['Check VP Population'] = np.where(((data['VP Delta'] >= 0.5) | (data['VP Delta'] <= -0.5)), 1, 0) #We consider updated everything modified in current or past year)\n",
        "    check_vp = data['Check VP Population'].sum()\n",
        "\n",
        "    #Select Check Popolation\n",
        "    processes = data.loc[data['Check EE Population'] == 1] \n",
        "    \n",
        "    #Join analysis in table\n",
        "    #del dq_report\n",
        "    dq_report = data_completeness.join(data_validity).join(missing_data).join(zero_values).join(unique_values).join(minimum_values).join(maximum_values)\n",
        "    #Conditional Formating\n",
        "    kpi = 98\n",
        "\n",
        "    #Color highlighting\n",
        "    def highlighter(cell_value):\n",
        "\n",
        "        highlight = 'background-color: yellow;'\n",
        "        default = 'background-color: lightgreen;'\n",
        "\n",
        "        if type(cell_value) in [float, int]:\n",
        "            if cell_value < kpi:\n",
        "                return highlight\n",
        "        return default\n",
        "\n",
        "    def highlighter_complete(cell_value):\n",
        "\n",
        "        highlight = 'background-color: yellow;'\n",
        "        default = 'background-color: lightgreen;'\n",
        "\n",
        "        if type(cell_value) in [float, int]:\n",
        "            if cell_value == len(data):\n",
        "                return default\n",
        "        return highlight\n",
        "\n",
        "    def highlighter_missing(cell_value):\n",
        "\n",
        "        highlight = 'background-color: yellow;'\n",
        "        default = 'background-color: lightgreen;'\n",
        "\n",
        "        if type(cell_value) in [float, int]:\n",
        "            if cell_value == 0:\n",
        "                return default\n",
        "        return highlight\n",
        "\n",
        "    dq_report=(\n",
        "        dq_report\n",
        "        .style\n",
        "        .set_table_styles([dict(props=[('max-width', '1200px')])])\n",
        "        .applymap(highlighter,  subset=['Validity %'])\n",
        "        .applymap(highlighter_missing,  subset=['Missing Values', 'Zero Values'])\n",
        "        .applymap(highlighter_complete, subset=['Completeness'])\n",
        "\n",
        "    )\n",
        "    \n",
        "    #Print table to image\n",
        "    dfi.export(dq_report, \"dq_report.png\")\n",
        "    \n",
        "    #Create full data report\n",
        "    full_data = pd.DataFrame(\n",
        "        {'Completeness %': completeness, 'Validity %': validity, 'Nullity %':nullity,'Uniqueness %':duplicates,\n",
        "         'Timeliness %':timeliness, 'Check EE Population': check_ee, 'Check VP Population': check_vp}, index=[0]\n",
        "    )\n",
        "    full_data = full_data.style.hide_index().format('{:.2f}')\n",
        "    \n",
        "    #Conditional formating\n",
        "    full_data=(\n",
        "        full_data\n",
        "        .applymap(highlighter,  subset=['Completeness %', 'Validity %', 'Uniqueness %', 'Timeliness %'])\n",
        "        .applymap(highlighter_missing,  subset=['Nullity %']))\n",
        "    \n",
        "    #Print table to image\n",
        "    dfi.export(full_data, \"full_data.png\")\n",
        "    \n",
        "    #Create pdf report\n",
        "\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "\n",
        "    #Add Title\n",
        "    title = 'Data Quality Profiler'\n",
        "    pdf.set_title(title)\n",
        "    # Arial bold 15\n",
        "    pdf.set_font('Arial', 'B', 14)\n",
        "    # Calculate width of title and position\n",
        "    w = pdf.get_string_width(title) + 6\n",
        "    pdf.set_x((210 - w) / 2)\n",
        "    # Colors of frame, background and text\n",
        "    pdf.set_draw_color(255, 255, 255)\n",
        "    pdf.set_fill_color(255, 255, 255)\n",
        "    pdf.set_text_color(65,105,225)\n",
        "    # Thickness of frame (1 mm)\n",
        "    pdf.set_line_width(1)\n",
        "    # Add title\n",
        "    pdf.cell(w, 9, title, 1, 1, 'C', 1)\n",
        "    # Line break\n",
        "    pdf.ln(4)\n",
        "\n",
        "    #Data Table\n",
        "    pdf.set_font('Arial', 'B', 10)\n",
        "    pdf.set_text_color(30,144,255)\n",
        "    pdf.cell(0, 5, 'Quality Profiling', 1, 1, 'L')\n",
        "    pdf.ln(1)\n",
        "    pdf.image('full_data.png', w = 120, h = 8)\n",
        "    pdf.ln(3)\n",
        "\n",
        "    #Column Table\n",
        "    pdf.set_font('Arial', 'B', 10)\n",
        "    pdf.set_text_color(30,144,255)\n",
        "    pdf.cell(0, 5, 'Quality Profiling by Column', 1, 1, 'L')\n",
        "    pdf.ln(1)\n",
        "    pdf.image('dq_report.png', w = 135,h = 150)\n",
        "    pdf.ln(3)\n",
        "\n",
        "    #Missing Graph\n",
        "    pdf.set_font('Arial', 'B', 10)\n",
        "    pdf.set_text_color(30,144,255)\n",
        "    pdf.cell(0, 5, 'Missing Data Matrix', 1, 1, 'L')\n",
        "    pdf.ln(3)\n",
        "    pdf.image('missing.png', w = 100, h = 50)\n",
        "\n",
        "    #Dimension explanation\n",
        "    pdf.add_page()\n",
        "    text = \"\"\"\n",
        "    Top data quality dimensions to assess:\n",
        "    Completess: filled-out values, even with blanks or zeros.\n",
        "    Nullity: missing data with null/nan values.\n",
        "    Uniqueness: number of unique values with no duplicates.\n",
        "    Validity: number of unique values with the correct format (object, float, datetime, etc).\n",
        "    Timeliness: information available when it is required. In this instance, it refers to the % of processes updated in the past 2 years.\n",
        "    Other dimensions for further analysis:\n",
        "    Accuracy: it describes how well does a piece of information reflect reality. It can be validated by process owners or decision makers in general.\n",
        "    Consistency: values that match values elsewhere.  For instance, elements of a form or tool.\n",
        "    \"\"\"\n",
        "    pdf.set_font('Arial', '', 7)\n",
        "    pdf.set_text_color(0,0,0)\n",
        "    pdf.multi_cell(0, 3, text)\n",
        "\n",
        "    pdf.output('QualityReport.pdf', 'F')\n",
        "    \n",
        "    #Print data to CSV\n",
        "    data.to_csv('processchecklist.csv')\n",
        "    \n",
        "    profile = ProfileReport(data, title=\"Quality Profiling Report\")\n",
        "    \n",
        "    return profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df7c84d7",
      "metadata": {
        "id": "df7c84d7"
      },
      "outputs": [],
      "source": [
        "def full_profile_variable(profile):\n",
        "    #profile = ProfileReport(data, title=\"Quality Profiling Report\")\n",
        "    profile.to_file(\"fulldataqualityreport.html\")\n",
        "    return profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8248e7f5",
      "metadata": {
        "id": "8248e7f5"
      },
      "outputs": [],
      "source": [
        "def full_investigation_by_column(data):\n",
        "    #Investigate Unique values for specific columns\n",
        "    #Indicate column\n",
        "    column= 'Employees Served'\n",
        "    data.groupby(column)['Process ID'].nunique()\n",
        "    #Check for updated data by year\n",
        "    #Select date field\n",
        "    date = 'Year'\n",
        "    data.groupby(date)['Process ID'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd55851a",
      "metadata": {
        "id": "cd55851a"
      },
      "source": [
        "# Main modules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d5fa1dc",
      "metadata": {
        "id": "1d5fa1dc"
      },
      "source": [
        "### Loads only df1 and df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3b791b4",
      "metadata": {
        "scrolled": true,
        "id": "c3b791b4",
        "outputId": "addceac5-6550-4a1e-ff83-70fd3a5b87b5",
        "colab": {
          "referenced_widgets": [
            "15b5679ed6514329a8e40161f17a14fc",
            "45f6508fd0654f34b62334e80bf56ab2",
            "f26f568965974323937a3e03106cd49a",
            "003150e6e71d4849831937b7b27bc804"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15b5679ed6514329a8e40161f17a14fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\users\\möebius\\documents\\prisma venv\\aivenv\\lib\\site-packages\\numpy\\core\\_methods.py:202: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = asanyarray(arr - arrmean)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45f6508fd0654f34b62334e80bf56ab2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f26f568965974323937a3e03106cd49a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "003150e6e71d4849831937b7b27bc804",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "s1, s2 = load_data()\n",
        "rawdata = data_format(s1, s2)\n",
        "cdata = clean_data(rawdata)\n",
        "pdata = profiling_by_dimension(cdata)\n",
        "pfile = full_profile_variable(pdata)\n",
        "full_investigation_by_column(cdata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbfb4a7e",
      "metadata": {
        "id": "bbfb4a7e"
      },
      "source": [
        "### Load all datasets allocated at 'files' folder\n",
        "\n",
        "Instruction: You need to create the 'files' folder at root and save all the files there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0d67b4",
      "metadata": {
        "scrolled": true,
        "id": "3c0d67b4",
        "outputId": "b051c396-01fa-43db-aa7b-ef8ffa8df8b9",
        "colab": {
          "referenced_widgets": [
            "185a2c580252473fb47f0d3183afc9ae",
            "f9201b485e5943b8baa277a2fcdcc90e",
            "aab5b1c74cb64137bf2ddde7dd328fff",
            "ead399af33e045cb866fdc37c5d51b23"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sourcedata1.csv\n",
            "sourcedata2.csv\n",
            "sourcedata3.csv\n",
            "sourcedata4.csv\n",
            "sourcedata5.csv\n",
            "sourcedata6.csv\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "185a2c580252473fb47f0d3183afc9ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\users\\möebius\\documents\\prisma venv\\aivenv\\lib\\site-packages\\pandas_profiling\\model\\correlations.py:61: UserWarning: There was an attempt to calculate the cramers correlation, but this failed.\n",
            "To hide this warning, disable the calculation\n",
            "(using `df.profile_report(correlations={\"cramers\": {\"calculate\": False}})`\n",
            "If this is problematic for your use case, please report this as an issue:\n",
            "https://github.com/pandas-profiling/pandas-profiling/issues\n",
            "(include the error message: 'No data; `observed` has size 0.')\n",
            "  (include the error message: '{error}')\"\"\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9201b485e5943b8baa277a2fcdcc90e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aab5b1c74cb64137bf2ddde7dd328fff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ead399af33e045cb866fdc37c5d51b23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "datas = load_data_multiple()\n",
        "rawdata = data_format_multiple(datas)\n",
        "cdata = clean_data(rawdata)\n",
        "pdata = profiling_by_dimension(cdata)\n",
        "pfile = full_profile_variable(pdata)\n",
        "full_investigation_by_column(cdata)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "Data_Quality_sanitized[modular].ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9df15392"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}